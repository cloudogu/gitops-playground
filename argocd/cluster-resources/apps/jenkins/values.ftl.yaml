dockerClientVersion: ${config.jenkins.internalDockerClientVersion}

controller:
  image:
    registry: ghcr.io
    repository: cloudogu/jenkins-helm
    # The image corresponds to the helm version,
    # because it contains the default plugins for this particular chart version
    tag: "${config.jenkins.helm.version}"
  installPlugins: false
  
  # to prevent the jenkins-ui-test pod being created
  testEnabled: false

  serviceType: NodePort
  servicePort: 80

  jenkinsUrl: ${config.jenkins.url}

<#if config.application.baseUrl?has_content>
  ingress:
    enabled: true
    hostName: ${config.jenkins.ingress}

</#if>
  # Don't use controller for builds
  numExecutors: 0

  # controller and agents need to run on the same host. See comment above agent.workingDir for details.
  nodeSelector:
    node: jenkins

  runAsUser: 1000

  admin:
    # Use reproducible admin password from secret. Change there, if necessary.
    #createSecret: false
    existingSecret: jenkins-credentials

  containerEnv:
    - name: PATH
      # We already mounted this PATH on the controller-agent. Still, "docker.inside {}" fails in pipeline?
      # Why? The docker pipeline plugin seems to set an empty environment: https://github.com/jenkinsci/docker-workflow-plugin/blob/docker-workflow-1.25/src/main/java/org/jenkinsci/plugins/docker/workflow/client/DockerClient.java#L261
      # Workaround: Set the ENV in the container:
      value: /opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tmp/docker

  customInitContainers:
    # Create Jenkins agent working dir explicitly. Otherwise, it seems to be owned by root
    # The does not need this folder, but creating it in a defined way and ownership on startup is better than to run an
    # InitContainer on each agent startup
    - name: create-agent-working-dir
      securityContext:
        runAsUser: 1000
      <#-- We use the same image for several tasks for performance and maintenance reasons -->
      image: ${config.jenkins.internalBashImage}
      imagePullPolicy: "{{ .Values.controller.imagePullPolicy }}"
      command: [ "/usr/local/bin/bash", "-c" ]
      args:
        - set -x -o nounset -o pipefail -o errexit;

          id;
          if [[ ! -d /host-tmp/gitops-playground-jenkins-agent ]]; then
          echo creating /tmp/gitops-playground-jenkins-agent on host and chowning to UID 1000;
          mkdir /host-tmp/gitops-playground-jenkins-agent;
          fi;

          if [[ -f /host-tmp/docker/docker ]]; then echo 'Docker already installed'; exit 0; fi;
          cd /host-tmp;
          wget -q https://download.docker.com/linux/static/stable/x86_64/docker-{{.Values.dockerClientVersion}}.tgz -O docker.tgz;
          tar -xzf docker.tgz;
          rm docker.tgz;
          find docker -type f -not -name 'docker' -delete;
          # Delete containerd, etc. We only need the docker CLI
          # Note: "wget -O- | tar" leads to the folder being owned by root, even when creating it beforehand?!
      volumeMounts:
        - name: host-tmp
          mountPath: /host-tmp

persistence:
  volumes:
    # Needed for initContainer only
    - name: host-tmp
      hostPath:
        path: /tmp

agent:
  # In our local playground infrastructure, builds are run in agent containers (pods).
  # During the builds, more containers are started via the Jenkins Docker Plugin (on the same docker host).
  # This leads to a scenario where the agent container tries to mount its filesystem into another container.
  # The docker host is only able to realize these mounts when the mounted paths are the same inside and outside the
  # containers.
  # So as a workaround, we provide the path inside the container also outside the container.
  # The /tmp folder is a good choice because it is writable for all users on the host.
  # One disadvantage is that /tmp is deleted when the host shuts down. 
  # This might slow down builds.
  # A different option would be to link the workspace into this repo.
  # If we should ever want to implement this, the logic can be reused from Git History:
  # https://github.com/cloudogu/gitops-playground/blob/61e033/scripts/apply.sh#L211-L235
  # We mount the same PATH as a hostPath. See below.
  # On Multi Node Clusters this leads to the requirement that Jenkins controller and agents run on the same host
  # We realize this using nodeSelectors 
  workingDir: "/tmp/gitops-playground-jenkins-agent"
  <#-- Note that setting the user as int seems to the value being ignored either by the helm chart or eventually by CasC plugin -->
  runAsUser: <#if dockerGid?has_content>1000<#else>"0"</#if>
  runAsGroup: <#if dockerGid?has_content>${dockerGid}<#else>"133"</#if>
  nodeSelector:
    node: jenkins
  # Number of concurrent builds. Keep it low to avoid high CPU load.
  containerCap: 2
  customJenkinsLabels: [ 'docker' ]
  resources:
    limits:
      cpu: "1"
      memory: "4Gi"
  envVars:
    - name: PATH
      # Add /tmp/docker to the path
      value: /opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tmp/docker
  volumes:
    - type: HostPath
      # See workingDir
      hostPath: /tmp/gitops-playground-jenkins-agent
      mountPath: /tmp/gitops-playground-jenkins-agent
    - type: HostPath
      # Persist Jenkins-home on build-agents, e.g. for maven repos
      # That is, muuuch faster builds after the first one
      hostPath: /tmp/gitops-playground-jenkins-agent
      mountPath: /home/jenkins
    - type: HostPath
      # When run locally, allow jenkins controller to access docker client
      hostPath: /var/run/docker.sock
      mountPath: /var/run/docker.sock
    - type: HostPath
      # Use a static docker binary.
      # It's downloaded by the jenkins controller's init container, so it needs to be done only once.
      hostPath: /tmp/docker/
      mountPath: /tmp/docker/
  # Controls how agent pods are retained after the Jenkins build completes
  # Not in Jenkins but in K8s. Helpful for debugging
  # In order to get rid of those many "default-xyz" pod use: kubectl delete pod -l jenkins/jenkins-jenkins-agent=true
  podRetention: "Always"